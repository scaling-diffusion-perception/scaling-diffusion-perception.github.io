<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Scaling Diffusion Perception</title>
    <link rel="stylesheet" href="style_2.css">
</head>
<body>
    <div id="title_slide">
        <div class="title_left">
            <h1>Scaling Properties of Diffusion Models For Perceptual Tasks</h1>
            <div class="author-container-1">
                <div class="grid-item"><a href="">Rahul Ravishankar*</a></div>
                <div class="grid-item"><a href="https://zeeshanp.me">Zeeshan Patel*</a></div>
                <div class="grid-item"><a href="https://brjathu.github.io">Jathushan Rajasegaran</a></div>
                <div class="grid-item"><a href="https://people.eecs.berkeley.edu/~malik/">Jitendra Malik</a></div>
            </div>
            <div class="mobile-author-container-1">
                <div class="grid-item"><a href="">Rahul Ravishankar*</a></div>
                <div class="grid-item"><a href="https://zeeshanp.me">Zeeshan Patel*</a></div>
                <div class="grid-item"><a href="https://brjathu.github.io">Jathushan Rajasegaran</a></div>
                <div class="grid-item"><a href="https://people.eecs.berkeley.edu/~malik/">Jitendra Malik</a></div>
            </div>
            <div class="berkeley">
                <p>University of California, Berkeley</p>
            </div>
            <div class="berkeley">
                <p>*Equal Contribution</p>
            </div>
            <div class="button-container">
                <a href="https://arxiv.org/abs/2411.08034" class="button">Paper</a>
                <a href="https://github.com/scaling-diffusion-perception/scaling-diffusion-perception" class="button">Code</a>
            </div>
    
            <br>
    
            <div id="abstract" class="grid-container">
                <p>
                In this paper, we argue that iterative computation with diffusion models offers a powerful paradigm for not only generation but also visual perception tasks. We unify tasks such as depth estimation, optical flow, and amodal segmentation under the framework of image-to-image translation, and show how diffusion models benefit from scaling training and test-time compute for these perceptual tasks. Through a careful analysis of these scaling properties, we formulate compute-optimal training and inference recipes to scale diffusion models for visual perception tasks. Our models achieve competitive performance to state-of-the-art methods using significantly less data and compute.                </p>
            </div>
        </div>
    </div>

    <!-- Divider Line -->
    <hr class="rounded">

    <!-- Overview Section -->
    <div id="overview">
        <h1>Scaling Diffusion Models For Perceptual Tasks</h1>
        <p>
            We display the effectiveness of leveraging iterative feedback computation through diffusion models for visual perception tasks. We perform an in-depth study of train/test-time compute scaling laws across all layers
            of the stack, including pre-training, fine-tuning, and diffusion inference. Specifically, we perform our study on the monocular depth estimation task. We show how to transfer the scaling laws derived for depth estimation to boost performance
            on tasks such as optical flow or amodal segmentation for both training and inference. Finally, we apply all of our scaling strategies to efficiently train a generalist mixture-of-experts model on perception tasks, achieving state-of-the-art results across various benchmarks.
        </p>
        <!-- Image Container -->
        <div class="barplot">
        <div class="image_container">
            <img src="assets/method.png" alt="Scaling Diffusion Method" class="overview_image">
            <div class="caption", style="text-align: justify;">
                <p> Figure 1: <b>A Unified Framework</b>: We fine-tune pre-trained Diffusion Models (DM) for visual
                    perception tasks. The input is a RGB image, a conditional image (i.e. next video frame, occlusion
                    mask, etc.), and a noised image of the ground truth prediction. Our model then generates predictions for various visual tasks such as monocular depth estimation, optical flow prediction, and
                    amodal segmentation, based on the conditional task embedding. We train a generalist model that can perform all three tasks with exceptional performance.</p>
            </div>
        </div>
    </div>
    </div>
    </div>

    <!-- Divider Line -->
    <br />
    <hr class="rounded">

    <div id="overview">
        <h1>Scaling Training Compute</h1>
        <p>We derive scaling laws for generative pre-training and fine-tuning of diffusion models for perceptual tasks. We pre-train DiT models of varied sizes on the ImageNet-1K dataset for class-conditional image generation. We observe clear power law scaling behavior as we increase the model size by increasing the hidden dimension and number of layers linearly.</p>
        <!-- Image Container -->
        <div class="barplot">
        <div class="image_container">
            <img src="assets/scaling_pretraining.png" alt="Scaling Diffusion Pretraining" style="max-width: 90%;">
            <div class="caption", style="text-align: center;">
                <p>Figure 2: Scaling law for generative pre-training of DiT on ImageNet-1K dataset.</p>
            </div>
        </div>
    </div>
        <br />
        <p>In addition to pre-training, we also derive scaling laws for fine-tuning on the downstream task of monocular depth estimation. We fine-tune the pre-trained DiT models by posing the depth estimation task as an image-to-image translation. We fine-tune our models for conditional denoising diffusion generation, training on the Hypersim dataset. We show that larger
           dense DiT models predictably converge to a lower fine-tuning loss. We also observe a strong correlation between the fine-tuning loss scaling law and validation metric scaling laws.</p>
        <!-- Image Container -->
        <div class="barplot">
        <div class="image_container">
            <img src="assets/scaling_finetuning.png" alt="Scaling Diffusion Fine-Tuning" style="width: 100%;">
            <div class="caption",  style="text-align: center;">
                <p>Figure 3: Scaling laws for fine-tuning of DiT for monocular depth estimation with the Hypersim dataset.</p>
            </div>
        </div>
        </div>
        <p>Finally, we also explore the effect of scaling <u>pre-training compute</u>, <u>image resolution</u>, and <u>mixture-of-experts upcyling</u> during fine-tuning. These results can be found in our <a href="#", style="color:#53aecf">paper</a>.</p>
    </div>

    <!-- Divider Line -->
    <br />
    <hr class="rounded">

    <div id="overview">
        <h1>Scaling Test-Time Compute</h1>
        <p>Scaling test-time compute has been explored for autoregressive LLMs to improve performance on long-horizon reasoning tasks.
            Diffusion models by design allow efficient scaling of test-time compute. First, we can simply increase the number of denoising 
            steps to increase the compute spent at inference. Since we are estimating deterministic outputs, we can then initialize multiple 
            noise latents and ensemble the predictions to get a better estimation. Finally, we can also reallocate test-time compute for low 
            and high frequency denoising by modifying the noise variance schedule.
        </p>
        <!-- Image Container -->
        <div class="barplot">
        <div class="image_container">
            <img src="assets/scaling_inference.png" alt="Scaling Diffusion Test-Time Compute" style="width: 100%;">
            <div class="caption", style="text-align: center">
                <p>Figure 4: Techniques to scale diffusion test-time compute for perceptual tasks.</p>
            </div>
        </div>
        </div>
        <br />
        <p>The most natural way of scaling diffusion inference is by increasing denoising steps. Since the model
            is trained to denoise the input at various timesteps, we can scale the number of diffusion denoising
            steps at test-time to produce finer, more accurate predictions. This coarse-to-fine denoising paradigm
            is also reflected in the generative case, and we can take advantage of it for the discriminative case
            by increasing the number of denoising steps. We show a clear power law scaling behavior in depth estimation
            validation metrics by simply increasing the number of diffusion sampling steps at test-time.</p>
        <div class="barplot">
        <div class="image_container">
            <img src="assets/scaling_inference_steps.png" alt="Scaling Diffusion Test-Time Compute (Steps)" style="width: 100%;">
            <div class="caption", style="text-align: center">
                <p>Figure 5: Scaling law for increasing denoising steps on model fine-tuned for depth estimation.</p>
            </div>
        </div>
        </div>
        <p>
        We can also exploit the fact that denoising different noise latents will generate different downstream predictions. We do so through a test-time ensembling approach in which we compute \( N \) forward passes per input sample and reduce the samples through an iterative optimization procedure. Since it
        requires no ground truth, we scale ensembling by increasing \( N \) to utilize more test-time compute. We apply test-time ensembling with \( N âˆˆ [1, 2, 5, 10, 15, 20] \). We show that ensembling multiple predictions from distinct noise initializations displays power law scaling behavior for depth estimaton.
        </p>
        <div class="barplot">
        <div class="image_container">
            <img src="assets/scaling_inference_ensembling.png" alt="Scaling Diffusion Test-Time Compute (ensembling)" style="width: 100%;">
            <div class="caption", style="text-align: center">
                <p>Figure 6: Scaling law for test-time ensembling on model fine-tuned for depth estimation.</p>
            </div>
        </div>
        </div>
        <p>
            Finally, we can scale test-time compute by increasing compute usage at different points of the denoising
            process. In diffusion noise schedulers, we can define a schedule for the variance of the Gaussian
            noise applied to the image over the total diffusion timesteps \( T \). Tuning the noise variance schedule
            allows for reorganizing compute by allocating more compute to denoising steps earlier or later in
            the noise schedule. We experiment with three different noise level settings for DDIM: linear, scaled
            linear, and cosine. Cosine scheduling inearly declines from the middle of the corruption process, ensuring
            the image is not corrupted too quickly as in linear schedules. Figure 7 shows that the cosine noise variance 
            schedule outperforms linear schedules for DDIM on the depth estimation task under a fixed compute budget.
        </p>
        <div class="barplot">
        <div class="image_container">
            <img src="assets/scaling_inference_noisevar.png" alt="Scaling Diffusion Test-Time Compute (ensembling)" style="width: 100%;">
            <div class="caption", style="text-align: center">
                <p>Figure 7: Reallocating test-time compute with noise variance schedule on models fine-tuned for depth estimation.</p>
            </div>
        </div>
        </div>
    </div>

    <!-- Divider Line -->
    <br />
    <hr class="rounded">

    <div id="overview">
        <h1>Putting It All Together</h1>
        <p>We train a unified generalist model capable of performing depth estimation, optical flow estimation, and amodal segementation tasks. We apply all of our training and inference scaling techniques, highlighting the generalizability of our approach.</p>
        <div class="barplot">
        <div class="image_container">
            <img src="assets/predictions.png" alt="Generations" style="width: 100%;">
            <div class="caption", style="text-align: center">
                <p>Figure 8: Generations for depth estimation, optical flow estimation, and amodal segmentation from our generalist model.</p>
            </div>
        </div>
        </div>
        <p>To train our generalist model, we modify the DiT-XL architecture by replacing the patch embedding layer with a separate \( \verb|PatchEmbedRouter| \) module, which
            routes each VAE embedding to a specific input convolutional layer based perception task. This
            ensures the DiT-XL model is able to distinguish between the task-specific embeddings during fine-
            tuning. After the initial fine-tuning of the dense model, we upcycle it to a DiT-XL-8E2A model and continue fine-tuning the new mixture-of-experts model.
        </p>
        <br />
        <p>
            Our results prove the effectiveness of our training and test-time scaling strategies, removing the need to use pre-trained models trained on internet-scale datasets to enable high-quality visual perception in diffusion models. We hope to inspire future work in scaling training
            and test-time compute for iterative generative paradigms.
        </p>
        <br />
        <h1>BibTeX</h1>
        <p class="bibtex">
            @article{ScalingDiffusionPerception2024,<br>
            &nbsp;&nbsp;title={Scaling Properties of Diffusion Models for Perceptual Tasks},<br>
            &nbsp;&nbsp;author={Rahul Ravishankar and Zeeshan Patel and Jathushan Rajasegaran and Jitendra Malik},<br>
            &nbsp;&nbsp;year={2024},<br>
            &nbsp;&nbsp;journal={arXiv:2411.08034}<br>
            &nbsp;&nbsp;url={https://arxiv.org/abs/2411.08034}<br>
            }
        </p>
    </div>
    <br />
    <br />
    <br />
    <br />

    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" async></script>

</body>
</html>