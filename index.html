<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Scaling Diffusion Perception</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <div class="container">
        <h1>Scaling Properties of Diffusion Models For Perceptual Tasks</h1>
        <div class="authors">
            <p>Rahul Ravishankar* &nbsp; | &nbsp; Zeeshan Patel* &nbsp; | &nbsp; Jathushan Rajasegaran &nbsp; | &nbsp; Jitendra Malik</p>
            <p>University of California, Berkeley</p>
        </div>
        <div class="buttons">
            <button onclick="window.location.href='#'">Paper</button>
            <button onclick="window.location.href='#'">Code</button>
        </div>
        <div class="abstract">
            <p>
                In this paper, we argue that iterative computation with diffusion models offers a powerful paradigm for not only generation but also visual perception tasks. We unify tasks such as depth estimation, optical flow, and segmentation under image-to-image translation, and show how diffusion models benefit from scaling training and test-time compute for these perception tasks. Through a careful analysis of these scaling behaviors, we present various techniques to efficiently train diffusion models for visual perception tasks. Our models achieve improved or comparable performance to state-of-the-art methods using significantly less data and compute.
            </p>
        </div>
    </div>
</body>
</html>
